{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ic_module.py",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1zjDvoGDu1KfAczcRfF4QRwBPN8XPSg-k",
      "authorship_tag": "ABX9TyMDo9OvSw9oa4BfSWjVRdV8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fuku-nao/0826-kadai/blob/master/ic_module.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuudM0sISuTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! -*- coding: utf-8 -*-\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
        "from keras.preprocessing.image import random_rotation, random_shift, random_zoom\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.models import model_from_json\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "\n",
        "FileNames = [\"img1.npy\", \"img2.npy\", \"img3.npy\"]\n",
        "ClassNames = [\"hashimoto_kannna\", \"horikita_maki\", \"sasaki_nozomi\"]\n",
        "hw = {\"height\":32, \"width\":32}        # リストではなく辞書型 中かっこで囲む\n",
        "\n",
        "google_drive_dir = \"/content/drive/My Drive/gazoubunrui/\"\n",
        "\n",
        "\n",
        "################################\n",
        "###### 画像データの前処理 ######\n",
        "################################\n",
        "def PreProcess(dirname, filename, var_amount=3):\n",
        "    num = 0\n",
        "    arrlist = []\n",
        "    files = glob.glob(dirname + \"/*.jpg\")\n",
        "\n",
        "    for imgfile in files:\n",
        "        img = load_img(imgfile, target_size=(hw[\"height\"], hw[\"width\"]))    # 画像ファイルの読み込み\n",
        "        array = img_to_array(img) / 255                                     # 画像ファイルのnumpy化\n",
        "        arrlist.append(array)                 # numpy型データをリストに追加\n",
        "        # for i in range(var_amount-1):\n",
        "        #    arr2 = array\n",
        "        #    arr2 = random_rotation(arr2, rg=360)\n",
        "        #    arrlist.append(arr2)              # numpy型データをリストに追加\n",
        "        num += 1\n",
        "\n",
        "    nplist = np.array(arrlist)\n",
        "    np.save(google_drive_dir + filename, nplist)\n",
        "    print(\">> \" + dirname + \"から\" + str(num) + \"個のファイル読み込み成功\")\n",
        "\n",
        "\n",
        "################################\n",
        "######### モデルの構築 #########\n",
        "################################\n",
        "def BuildCNN(ipshape=(32, 32, 3), num_classes=3):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(24, 3, padding='same', input_shape=ipshape))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(48, 3))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Conv2D(96, 3, padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(96, 3))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=adam,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "################################\n",
        "############# 学習 #############\n",
        "################################\n",
        "def Learning(tsnum=30, nb_epoch=50, batch_size=8, learn_schedule=0.9):\n",
        "    X_TRAIN_list = []; Y_TRAIN_list = []; X_TEST_list = []; Y_TEST_list = [];\n",
        "    target = 0\n",
        "    for filename in FileNames :\n",
        "        data = np.load(google_drive_dir + filename)          # 画像のnumpyデータを読み込み\n",
        "        trnum = data.shape[0] - tsnum\n",
        "        X_TRAIN_list += [data[i] for i in range(trnum)]          # 画像データ\n",
        "        Y_TRAIN_list += [target] * trnum                         # 分類番号\n",
        "        X_TEST_list  += [data[i] for i in range(trnum, trnum+tsnum)]          # 学習しない画像データ\n",
        "        Y_TEST_list  += [target] * tsnum;                                     # 学習しない分類番号\n",
        "        target += 1\n",
        "\n",
        "    X_TRAIN = np.array(X_TRAIN_list + X_TEST_list)    # 連結\n",
        "    Y_TRAIN = np.array(Y_TRAIN_list + Y_TEST_list)    # 連結\n",
        "    print(\">> 学習サンプル数 : \", X_TRAIN.shape)\n",
        "    y_train = np_utils.to_categorical(Y_TRAIN, target)    # 自然数をベクトルに変換\n",
        "    valrate = tsnum * target * 1.0 / X_TRAIN.shape[0]\n",
        "\n",
        "    # 学習率の変更\n",
        "    class Schedule(object):\n",
        "        def __init__(self, init=0.001):      # 初期値定義\n",
        "            self.init = init\n",
        "        def __call__(self, epoch):           # 現在値計算\n",
        "            lr = self.init\n",
        "            for i in range(1, epoch+1):\n",
        "                lr *= learn_schedule\n",
        "            return lr\n",
        "\n",
        "    def get_schedule_func(init):\n",
        "        return Schedule(init)\n",
        "\n",
        "    lrs = LearningRateScheduler(get_schedule_func(0.001))\n",
        "    mcp = ModelCheckpoint(filepath='best.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "    model = BuildCNN(ipshape=(X_TRAIN.shape[1], X_TRAIN.shape[2], X_TRAIN.shape[3]), num_classes=target)\n",
        "\n",
        "    print(\">> 学習開始\")\n",
        "    hist = model.fit(X_TRAIN, y_train,\n",
        "                     batch_size=batch_size,\n",
        "                     verbose=1,\n",
        "                     epochs=nb_epoch,\n",
        "                     validation_split=valrate,\n",
        "                     callbacks=[lrs, mcp])\n",
        "\n",
        "    json_string = model.to_json()\n",
        "    json_string += '##########' + str(ClassNames)\n",
        "    open(google_drive_dir + 'model.json', 'w').write(json_string)\n",
        "    model.save_weights(google_drive_dir + 'last.hdf5')\n",
        "\n",
        "\n",
        "################################\n",
        "########## 試行・実験 ##########\n",
        "################################\n",
        "def TestProcess(imgname):\n",
        "    modelname_text = open(google_drive_dir + \"model.json\").read()\n",
        "    json_strings = modelname_text.split('##########')\n",
        "    textlist = json_strings[1].replace(\"[\", \"\").replace(\"]\", \"\").replace(\"\\'\", \"\").split()\n",
        "    print(\"1\")\n",
        "    model = model_from_json(json_strings[0])\n",
        "    print(\"2\")\n",
        "    model.load_weights(google_drive_dir + \"last.hdf5\")  # best.hdf5 で損失最小のパラメータを使用\n",
        "    print(\"3\")\n",
        "    img = load_img(imgname, target_size=(hw[\"height\"], hw[\"width\"]))    \n",
        "    TEST = img_to_array(img) / 255\n",
        "\n",
        "    pred = model.predict(np.array([TEST]), batch_size=1, verbose=0)\n",
        "    print(\">> 計算結果↓\\n\" + str(pred))\n",
        "    print(\">> この画像は「\" + textlist[np.argmax(pred)].replace(\",\", \"\") + \"」です。\")\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_AU5GOnVVwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y47L4QTOVWDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l7v2QKlTs80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}